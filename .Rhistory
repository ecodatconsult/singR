2024-Mar-12,12:16:00,50.72196,N,7.50443,E,4.9,1
2024-Mar-12,12:20:00,50.72196,N,7.50443,E,4.9,0
2024-Mar-12,12:21:00,50.72196,N,7.50443,E,4.9,1", tmpfile)
# Test the function
formatted_data <- format_deployment(tmpfile, device = "songmeter")
# Check if the output has the correct structure and content
expect_s3_class(formatted_data, "sf")
expect_true(all(c("start_datetime", "end_datetime", "deployment_path") %in% names(formatted_data)))
expect_equal(nrow(formatted_data), 2)
expect_equal(ncol(formatted_data), 3)
})
test_that("format_deployment throws an error for unsupported devices", {
# Test if the function throws an error for unsupported device
expect_error(format_deployment("dummy.txt", device = "other_device"), "no other devices than songmeter supported")
})
devtools::test_active_file()
qe
)
devtools::test_active_file()
expect_equal(ncol(formatted_data), 4)
test_that("format_deployment returns the correct formatted deployment data for songmeter device", {
# Create a temporary TXT file for testing
tmpfile <- tempfile(fileext = ".txt")
writeLines("DATE,TIME,LAT,,LON,,POWER(V),#FILES
2024-Mar-08,07:34:00,50.72196,N,7.50443,E,5.0,0
2024-Mar-12,12:00:00,50.72196,N,7.50443,E,4.9,0
2024-Mar-12,12:01:00,50.72196,N,7.50443,E,4.9,1
2024-Mar-12,12:05:00,50.72196,N,7.50443,E,4.9,0
2024-Mar-12,12:06:00,50.72196,N,7.50443,E,4.9,1
2024-Mar-12,12:10:00,50.72196,N,7.50443,E,4.9,0
2024-Mar-12,12:11:00,50.72196,N,7.50443,E,4.9,1
2024-Mar-12,12:15:00,50.72196,N,7.50443,E,4.9,0
2024-Mar-12,12:16:00,50.72196,N,7.50443,E,4.9,1
2024-Mar-12,12:20:00,50.72196,N,7.50443,E,4.9,0
2024-Mar-12,12:21:00,50.72196,N,7.50443,E,4.9,1", tmpfile)
# Test the function
formatted_data <- format_deployment(tmpfile, device = "songmeter")
# Check if the output has the correct structure and content
expect_s3_class(formatted_data, "sf")
expect_true(all(c("start_datetime", "end_datetime", "deployment_path") %in% names(formatted_data)))
expect_equal(nrow(formatted_data), 1)
expect_equal(ncol(formatted_data), 4)
})
test_that("format_deployment throws an error for unsupported devices", {
# Test if the function throws an error for unsupported device
expect_error(format_deployment("dummy.txt", device = "other_device"), "no other devices than songmeter supported")
})
# Test if the function throws an error for unsupported device
expect_error(format_deployment("dummy.txt", device = "other_device"), "no other devices than songmeter supported")
devtools::test_active_file()
usethis::use_test("deployment_info")
# Create a temporary directory with test files
temp_dir <- tempdir()
dir.create(temp_dir)
# Create a sample _Summary.txt file
tmpfile <- file.path(temp_dir, "example_Summary.txt")
writeLines("DATE,TIME,LAT,,LON,,POWER(V),#FILES
2024-Mar-08,07:34:00,50.72196,N,7.50443,E,5.0,0
2024-Mar-12,12:00:00,50.72196,N,7.50443,E,4.9,0
2024-Mar-12,12:01:00,50.72196,N,7.50443,E,4.9,1", tmpfile)
# Mock the format_deployment function to avoid external dependencies
format_deployment_mock <- function(file, device) {
data <- read.csv(file)
data <- data.frame(
start_datetime = as.POSIXct(data$TIME, format = "%H:%M:%S", tz = "UTC"),
end_datetime = as.POSIXct(data$TIME, format = "%H:%M:%S", tz = "UTC") + 3600,
deployment_path = file
)
return(data)
}
# Replace format_deployment with the mocked version
assignInNamespace("format_deployment", format_deployment_mock, ns = asNamespace("your_package_name"))
# Replace format_deployment with the mocked version
assignInNamespace("format_deployment", format_deployment_mock, ns = asNamespace("singR"))
# Test the deployment_info function
deployments <- deployment_info(temp_dir)
# Check if the output has the correct structure and content
expect_s3_class(deployments, "data.frame")
expect_true(all(c("start_datetime", "end_datetime", "deployment_path", "deployment_id") %in% names(deployments)))
expect_equal(nrow(deployments), 3)
expect_equal(ncol(deployments), 4)
test_that("deployment_info returns correct deployment information") {
test_that("deployment_info returns correct deployment information") {
test_that("deployment_info returns correct deployment information",{
# Create a temporary directory with test files
temp_dir <- tempdir()
dir.create(temp_dir)
# Create a sample _Summary.txt file
tmpfile <- file.path(temp_dir, "example_Summary.txt")
writeLines("DATE,TIME,LAT,,LON,,POWER(V),#FILES
2024-Mar-08,07:34:00,50.72196,N,7.50443,E,5.0,0
2024-Mar-12,12:00:00,50.72196,N,7.50443,E,4.9,0
2024-Mar-12,12:01:00,50.72196,N,7.50443,E,4.9,1", tmpfile)
# Mock the format_deployment function to avoid external dependencies
format_deployment_mock <- function(file, device) {
data <- read.csv(file)
data <- data.frame(
start_datetime = as.POSIXct(data$TIME, format = "%H:%M:%S", tz = "UTC"),
end_datetime = as.POSIXct(data$TIME, format = "%H:%M:%S", tz = "UTC") + 3600,
deployment_path = file
)
return(data)
}
# Replace format_deployment with the mocked version
assignInNamespace("format_deployment", format_deployment_mock, ns = asNamespace("singR"))
# Test the deployment_info function
deployments <- deployment_info(temp_dir)
# Check if the output has the correct structure and content
expect_s3_class(deployments, "data.frame")
expect_true(all(c("start_datetime", "end_datetime", "deployment_path", "deployment_id") %in% names(deployments)))
expect_equal(nrow(deployments), 3)
expect_equal(ncol(deployments), 4)
})
# Clean up
test_that("deployment_info throws an error for non-existent directory", {
# Test if the function throws an error for non-existent directory
expect_error(deployment_info("non_existent_directory"), "cannot open the connection")
})
deployment_info("non_existent_directory")
# Test if the function throws an error for non-existent directory
expect_error(deployment_info("non_existent_directory"))
# Clean up
test_that("deployment_info throws an error for non-existent directory", {
# Test if the function throws an error for non-existent directory
expect_error(deployment_info("non_existent_directory"))
})
test_that("deployment_info returns correct deployment information",{
# Create a temporary directory with test files
temp_dir <- tempdir()
dir.create(temp_dir)
# Create a sample _Summary.txt file
tmpfile <- file.path(temp_dir, "example_Summary.txt")
writeLines("DATE,TIME,LAT,,LON,,POWER(V),#FILES
2024-Mar-08,07:34:00,50.72196,N,7.50443,E,5.0,0
2024-Mar-12,12:00:00,50.72196,N,7.50443,E,4.9,0
2024-Mar-12,12:01:00,50.72196,N,7.50443,E,4.9,1", tmpfile)
# Mock the format_deployment function to avoid external dependencies
format_deployment_mock <- function(file, device) {
data <- read.csv(file)
data <- data.frame(
start_datetime = as.POSIXct(data$TIME, format = "%H:%M:%S", tz = "UTC"),
end_datetime = as.POSIXct(data$TIME, format = "%H:%M:%S", tz = "UTC") + 3600,
deployment_path = file
)
return(data)
}
# Replace format_deployment with the mocked version
assignInNamespace("format_deployment", format_deployment_mock, ns = asNamespace("singR"))
# Test the deployment_info function
deployments <- deployment_info(temp_dir)
# Check if the output has the correct structure and content
expect_s3_class(deployments, "data.frame")
expect_true(all(c("start_datetime", "end_datetime", "deployment_path", "deployment_id") %in% names(deployments)))
expect_equal(nrow(deployments), 3)
expect_equal(ncol(deployments), 4)
})
usethis::use_test("extract_path_info")
usethis::use_test("create_id_from_path")
test_that("create_id_from_path creates correct ID at data level", {
# Test the function at data level
path <- "/project1/region1/location1/deployment1/data.csv"
id <- create_id_from_path(path, level = "data")
expect_equal(id, "12c7abbbcf8bf5d44a1fc9316f4de059")
})
# Test the function at data level
path <- "/project1/region1/location1/deployment1/data.csv"
id <- create_id_from_path(path, level = "data")
id
stringr::str_replace_all(path, "/", "_")
digest::digest(stringr::str_replace_all(path, "/", "_"), algo = "md5")
# Test the function at data level
path <- "/project1/region1/location1/deployment1/some_directory/data.csv"
digest::digest(stringr::str_replace_all(path, "/", "_"), algo = "md5")
# Test the function at data level
path <- "project1/region1/location1/deployment1/some_directory/data.csv"
digest::digest(stringr::str_replace_all(path, "/", "_"), algo = "md5")
digest::digest(stringr::str_replace_all(path, "/", "_"), algo = "md5")
id <- create_id_from_path(path, level = "data")
id
# Test the function at data level
path <- "project1/region1/location1/deployment1/some_directory/data.csv"
id <- create_id_from_path(path, level = "data")
expect_equal(id, "f3fb934f4107b3f99ee3eaf384d1d8b2")
# Test the function at deployment_id level
path <- "/project1/region1/location1/deployment1/data.csv"
id <- create_id_from_path(path, level = "deployment_id")
path
id
digest::digest("project1_region1_location1_deployment1", algo = "md5")
id <- create_id_from_path(path, level = "deployment_id")
id
# Test the function at deployment_id level
path <- "project1/region1/location1/deployment1/some_directory/data.csv"
id <- create_id_from_path(path, level = "deployment_id")
id
digest::digest("project1_region1_location1_deployment1", algo = "md5")
create_id_from_path
extract_path_info(path, level = "deployment_id")
extract_path_info(path, level = "data")
extract_path_info(path, level = "data_dir")
# Test the function at deployment_id level
path <- "project1/region1/location1/deployment1/"
extract_path_info(path, level = "data_dir")
digest::digest("project1_region1_location1_deployment1", algo = "md5")
id <- create_id_from_path(path, level = "deployment_id")
extract_path_info(path, level = "deployment_id")
# Test the function at deployment_id level
path <- "project1/region1/location1/deployment1/summary.txt"
extract_path_info(path, level = "deployment_id")
# Test the function at deployment_id level
path <- "project1/region1/location1/deployment1"
extract_path_info(path, level = "deployment_id")
# Test the function at deployment_id level
path <- "blabla/project1/region1/location1/deployment1"
extract_path_info(path, level = "deployment_id")
# Test the function at deployment_id level
path <- "blabla/blabla/project1/region1/location1/deployment1"
extract_path_info(path, level = "deployment_id")
# Test the function at data level
path <- "blabla/project1/region1/location1/deployment1/some_directory/data.csv"
id <- create_id_from_path(path, level = "data")
extract_path_info(path, level = "deployment_id")
digest::digest("project1_region1_location1_deployment1", algo = "md5")
id <- create_id_from_path(path, level = "deployment_id")
id
extract_path_info(path, level = "deployment_id")
# Test the function at deployment_id level
path <- "blabla/blabla/project1/region1/location1/deployment1"
extract_path_info(path, level = "deployment_id")
digest::digest("project1_region1_location1_deployment1", algo = "md5")
id <- create_id_from_path(path, level = "deployment_id")
id
# Test the function at deployment_id level
path <- "blabla/blabla/project1/region1/location1/deployment1/summary.txt"
path
id <- create_id_from_path(path, level = "deployment_id")
expect_equal(id, "5106fac0fecd29dc9c9c4fa433d30b2a")
id
devtools::test()
devtools::test()
devtools::test()
devtools::test()
devtools::load_all()
devtools::test()
devtools::test()
# Test the function at data level
path <- "/project1/region1/location1/deployment1/deployment1/data.csv"
info <- extract_path_info(path, level = "data")
expect_equal(info$data, "data.csv")
expect_equal(info$data_dir, "deployment1")
expect_equal(info$deployment_id, "deployment1")
expect_equal(info$location_id, "location1")
expect_equal(info$region_id, "region1")
expect_equal(info$project_id, "project1")
test_that("extract_path_info extracts correct path information at data level", {
# Test the function at data level
path <- "/project1/region1/location1/deployment1/deployment1/data.csv"
info <- extract_path_info(path, level = "data")
expect_equal(info$data, "data.csv")
expect_equal(info$data_dir, "deployment1")
expect_equal(info$deployment_id, "deployment1")
expect_equal(info$location_id, "location1")
expect_equal(info$region_id, "region1")
expect_equal(info$project_id, "project1")
})
path <- dirname(path)
info <- extract_path_info(path, level = "deployment_id")
expect_equal(info$deployment_id, "deployment1")
expect_equal(info$location_id, "location1")
path
# Test the function at deployment_id level
path <- "/project1/region1/location1/deployment1/data.csv"
path <- dirname(path)
info <- extract_path_info(path, level = "deployment_id")
expect_equal(info$deployment_id, "deployment1")
expect_equal(info$location_id, "location1")
expect_equal(info$region_id, "region1")
expect_equal(info$project_id, "project1")
devtools::test()
devtools::test()
devtools::load_all()
shinyRun()
devtools::load_all()
devtools::load_all()
db_location <- "/home/alex/Documents/databases/soundscape.gpkg"
if(!file.exists(db_location)){
sf::st_write(iris, dsn = db_location, "test_data")
sf::st_delete(layer = "test_data", dsn = db_location, driver = "GPKG")
sf::st_layers(db_location)
}
con <- RSQLite::dbConnect(RSQLite::SQLite(), db_location)
sf::st_layers("/home/alex/Documents/databases/soundscape.gpkg")
input_dir <- "/home/alex/Documents/Soundscape/KW2100"
# get deployment info
remaining_deployments <- deployment <- deployment_info(input_dir)
deployment_id_test <- deployment |>
dplyr::filter(stringr::str_detect(deployment_path, "/1/"))  |>
dplyr::pull(deployment_id)
deployment_id_test == digest::digest("KW2100/ADK/1/240312_240325", algo = "murmur32")
deployment |>
db_filter_upload(con = con,
table = "deployments",
idfields = "deployment_id")
# get
song_info(input_dir) |>
db_filter_upload(con = con,
table = "exif_metadata",
idfields = "data_id")
devtools::load_all()
db_location <- "/home/alex/Documents/databases/soundscape.gpkg"
if(!file.exists(db_location)){
sf::st_write(iris, dsn = db_location, "test_data")
sf::st_delete(layer = "test_data", dsn = db_location, driver = "GPKG")
sf::st_layers(db_location)
}
con <- RSQLite::dbConnect(RSQLite::SQLite(), db_location)
sf::st_layers("/home/alex/Documents/databases/soundscape.gpkg")
input_dir <- "/home/alex/Documents/Soundscape/KW2100"
# get deployment info
remaining_deployments <- deployment <- deployment_info(input_dir)
deployment_id_test <- deployment |>
dplyr::filter(stringr::str_detect(deployment_path, "/1/"))  |>
dplyr::pull(deployment_id)
deployment_id_test == digest::digest("KW2100/ADK/1/240312_240325", algo = "murmur32")
deployment |>
db_filter_upload(con = con,
table = "deployments",
idfields = "deployment_id")
deployment |>
dplyr::filter(paste(deployment_name, deployment_id, sep = "_") %in% list.dirs(basename(input_dir), "_birdnet"))
list.dirs(basename(input_dir), "_birdnet")
paste0(basename(input_dir), "_birdnet")
list.dirs(paste0(basename(input_dir), "_birdnet"))
paste0(input_dir, "_birdnet")
list.dirs(paste0(input_dir, "_birdnet"))
list.dirs(paste0(input_dir, "_birdnet"), recursive = FALSE)
deployment <- deployment |>
dplyr::filter(paste(deployment_name, deployment_id, sep = "_") %in% list.dirs(paste0(input_dir, "_birdnet"), recursive = FALSE))
deployment |>
db_filter_upload(con = con,
table = "deployments",
idfields = "deployment_id")
deployment <- deployment |>
dplyr::filter(paste(deployment_name, deployment_id, sep = "_") %in% basename(list.dirs(paste0(input_dir, "_birdnet"), recursive = FALSE)))
deployment |>
db_filter_upload(con = con,
table = "deployments",
idfields = "deployment_id")
# get deployment info
remaining_deployments <- deployment <- deployment_info(input_dir)
deployment_id_test <- deployment |>
dplyr::filter(stringr::str_detect(deployment_path, "/1/"))  |>
dplyr::pull(deployment_id)
deployment_id_test == digest::digest("KW2100/ADK/1/240312_240325", algo = "murmur32")
deployment |>
db_filter_upload(con = con,
table = "deployments",
idfields = "deployment_id")
deployment <- deployment |>
dplyr::filter(paste(deployment_name, deployment_id, sep = "_") %in% basename(list.dirs(paste0(input_dir, "_birdnet"), recursive = FALSE)))
# get deployment info
remaining_deployments <- deployment <- deployment_info(input_dir)
deployment_id_test <- deployment |>
dplyr::filter(stringr::str_detect(deployment_path, "/1/"))  |>
dplyr::pull(deployment_id)
deployment_id_test == digest::digest("KW2100/ADK/1/240312_240325", algo = "murmur32")
deployment |>
db_filter_upload(con = con,
table = "deployments",
idfields = "deployment_id")
deployment2 <- deployment |>
dplyr::filter(paste(deployment_name, deployment_id, sep = "_") %in% basename(list.dirs(paste0(input_dir, "_birdnet"), recursive = FALSE)))
basename(list.dirs(paste0(input_dir, "_birdnet"), recursive = FALSE))
deployment2 <- deployment |>
dplyr::filter(paste(deployment_name, deployment_id, sep = "__") %in% basename(list.dirs(paste0(input_dir, "_birdnet"), recursive = FALSE)))
deployment2 <- deployment |>
dplyr::filter(!paste(deployment_name, deployment_id, sep = "__") %in% basename(list.dirs(paste0(input_dir, "_birdnet"), recursive = FALSE)))
deployment <- deployment |>
dplyr::filter(!paste(deployment_name, deployment_id, sep = "__") %in% basename(list.dirs(paste0(input_dir, "_birdnet"), recursive = FALSE)))
for(deployment_id_sel in deployment$deployment_id){
print(Sys.time())
print(deployment_id_sel)
deployment_sel <- deployment |>
dplyr::filter(deployment_id == deployment_id_sel) |>
dplyr::filter(!duplicated(deployment_id))
coords <- deployment_sel$geometry |>
sf::st_centroid() |>
sf::st_coordinates()
week <- round(48 *lubridate::week(mean(c(deployment_sel$start_datetime, deployment_sel$end_datetime))) / 52)
temp_dir <- tempdir()
temp_dir_deployment <- paste0(temp_dir, "/", deployment_id_sel)
output_dir <- paste0(dirname(input_dir), "/", basename(input_dir), "_birdnet")
dir.create(output_dir)
dir.create(deployment_id_sel)
deployment_dir <- paste0(output_dir, "/", paste0(deployment_sel$deployment_name, "__", deployment_sel$deployment_id))
dir.create(deployment_dir)
# INITIALIZING BIRDNET ARGUMENT SETTINGS
birdnet_args_analyze <- list(i = deployment_sel$deployment_path,
o = temp_dir_deployment,
lon = coords[1],
lat = coords[2],
week = week,
slist = NA,
min_conf = 0.2,
overlap = 1.5,
rtype = "table",
threads = 4,
batchsize = 4,
locale = "de",
sf_thresh = 0.03,
classifier = NA,
fmin = 0,
fmax = 15000)
message(paste0(Sys.time(), ": running analysis"))
# ASSIGN ID TO SETTINGS BASED ON VALUES
birdnet_analyze_settings <- birdnet_args_analyze |>
data.frame()  |>
dplyr::mutate(birdnet_version = sort(basename(list.dirs("~/BirdNET-Analyzer/checkpoints/", recursive = FALSE)), decreasing = TRUE)[1]) |> #ASSUME LATEST INSTALLED VERSION IS USED
dplyr::select(-i, -o) %>%
dplyr::mutate(analyze_settings_id = apply(., 1, digest::digest, algo = "murmur32"),
.before = 1)
# UPLOAD SETTINGS TO LOCAL DB
birdnet_analyze_settings |>
db_filter_upload(con = con,
table = "birdnet_analyze_settings",
idfields = "analyze_settings_id")
# CHECK FOR PREVIOUS RUNS AND SETTINGS FOR THIS DEPLOYMENT
if("birdnet_analyze_results" %in% sf::st_layers(db_location)[[1]]){
previous_settings <- dplyr::tbl(con, "birdnet_analyze_results") |>
dplyr::filter(deployment_id == deployment_id_sel) |>
dplyr::select(analyze_settings_id) |>
dplyr::distinct() |>
dplyr::collect() |>
dplyr::pull()
}else{
previous_settings <- NA
}
settings_dir <- paste0(deployment_dir, "/"  , birdnet_analyze_settings$analyze_settings_id)
dir.create(settings_dir)
segments_dir <- paste0(settings_dir, "/segments/")
classifications_dir <- paste0(settings_dir, "/selected_Data/")
dir.create(segments_dir)
dir.create(classifications_dir)
# IF THESE ARE NEW SETTINGS ...
if(!birdnet_analyze_settings$analyze_settings_id %in% previous_settings){
# ... RUN BIRDNET
birdnet_analyze_results <- run_birdnet(mode = "analyze",
birdnet_loc = "~/BirdNET-Analyzer/",
birdnet_args = birdnet_args_analyze,
settings_id = birdnet_analyze_settings$analyze_settings_id)
birdnet_analyze_results |>
db_filter_upload(con = con,
table = "birdnet_analyze_results",
idfields = c("analyze_settings_id", "deployment_id"))
#
# message(paste0(Sys.time(), ": running embeddings"))
#
# birdnet_args_embeddings <-  list(i = input_dir,
#                                      o = temp_dir,
#                                      overlap = 1.5)
#
#   birdnet_embeddings_settings <- birdnet_args_embeddings |>
#     data.frame()  |>
#     dplyr::select(-i, -o) %>%
#     dplyr::mutate(embeddings_settings_id = apply(., 1, digest::digest, algo = "md5"),
#                   .before = 1)
#
#   birdnet_embeddings_settings |>
#     db_filter_upload(con = con,
#                      table = "birdnet_embeddings_settings",
#                      idfields = "embedding_settings_id",
#                      settings_id = birdnet_embeddings_settings)
#
#
#   birdnet_embeddings_results <- run_birdnet(mode = "embeddings",
#               birdnet_loc = "~/BirdNET-Analyzer/",
#               birdnet_args = birdnet_args_embeddings)
#
# generate segements with a 1 percent chance
message(paste0(Sys.time(), ": creating segments"))
files_with_best_detection <- birdnet_analyze_results |>
dplyr::group_by(common_name) |>
dplyr::filter(confidence == max(confidence)) |>
dplyr::filter(common_name != "nocall")
analysis_files <-  basename(files_with_best_detection$data_file) |>
tools::file_path_sans_ext() |>
paste0(".Bird") |>
sapply(function(temp_dir_deployment, pattern){
list.files(temp_dir_deployment, pattern = pattern, recursive = TRUE, full.names = TRUE)}, temp_dir_deployment = temp_dir_deployment) |>
lapply(function(x) x[1]) |>
unlist()
analysis_files2 <- list.files(temp_dir_deployment, pattern = "selection.table.txt", recursive = TRUE, full.names = TRUE)
file.copy(paste0(temp_dir_deployment, "/Data"), settings_dir, recursive = TRUE)
analysis_files_2_segment <-
c(
analysis_files,
analysis_files2[!analysis_files2 %in% analysis_files] %>%
sample(size = ceiling(length(.)/10))
)
unique_analysis_files <- analysis_files_2_segment |> unique()
file.copy(unique_analysis_files, classifications_dir)
run_birdnet(mode = "segments",
birdnet_loc = "~/BirdNET-Analyzer/",
birdnet_args = list(audio = deployment_sel$deployment_path,
results = classifications_dir,
o = segments_dir,
min_conf = 0.2,
seg_length = 4.5
))
create_birdnet_workbook(dir_path = settings_dir)
}
}
